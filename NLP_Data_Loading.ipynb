{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data to prepare for Topic Modeling\n",
    "\n",
    "This is the first notebook in the series of Topic Modeling notebooks\n",
    "\n",
    "Please note: open Jupyter Notebooks using Anaconda Prompt: jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "You will need this increaset data rate for NLP Data Processing. See more here: https://stackoverflow.com/questions/43288550/iopub-data-rate-exceeded-in-jupyter-notebook-when-viewing-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General Dependencies\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import sys, os\n",
    "import glob\n",
    "from tika import parser # pip install tika\n",
    "import inspect\n",
    "import datetime\n",
    "import pickle5 as pickle\n",
    "\n",
    "## Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import models\n",
    "#from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models import ldaseqmodel\n",
    "\n",
    "\n",
    "## Preprocessing\n",
    "import spacy\n",
    "import nltk as nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "## Plotting\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import ast\n",
    "\n",
    "## Other Libraries\n",
    "from operator import itemgetter\n",
    "\n",
    "## ScikitLearn\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening data: if data are a folder of full text PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the glob method to retrieve files/pathnames in the directory\n",
    "## https://www.geeksforgeeks.org/how-to-use-glob-function-to-find-files-recursively-in-python/\n",
    "\n",
    "directory = \"data/Interview_transcripts\"\n",
    "\n",
    "# ## Other file directories\n",
    "# ## \"data/10_News_All_Final\"\n",
    "# ## \"data/1_News_Industry\"\n",
    "# ## \"data/2_Government_Documents\"\n",
    "# ## \"data/3_Altmetric_Policy\"\n",
    "\n",
    "files = list(glob.glob(os.path.join(directory,'*.*')))\n",
    "\n",
    "## Inspect output as needed\n",
    "print(files)\n",
    "\n",
    "## Other Resources\n",
    "## https://stackoverflow.com/questions/34000914/how-to-create-a-list-from-filenames-in-a-user-specified-directory-in-python\n",
    "## https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "## https://stackoverflow.com/questions/33912773/python-read-txt-files-into-a-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract text from the pdfs and add them to a list using Tika Python\n",
    "## The output is a dictionary with: metadata, content, status\n",
    "\n",
    "document_list = []\n",
    "for f in files:\n",
    "    raw = parser.from_file(f)\n",
    "    document_list.append(raw)\n",
    "    \n",
    "## Resources\n",
    "## https://www.geeksforgeeks.org/parsing-pdfs-in-python-with-tika/\n",
    "## https://stackoverflow.com/questions/34837707/how-to-extract-text-from-a-pdf-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the dictionary to a pandas dataframe\n",
    "\n",
    "text_df = pd.DataFrame(document_list)\n",
    "\n",
    "## Inspect the output as needed\n",
    "# text_df.head()\n",
    "# print(text_df[\"metadata\"][1])\n",
    "# print(text_df[\"content\"][1])\n",
    "# text_df.to_csv('gensim_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count the number of rows in the dataframe\n",
    "\n",
    "text_df_count_row = text_df.shape[0]  # gives number of row count\n",
    "\n",
    "## Inspect output as needed\n",
    "print(text_df_count_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new column in the dataframe called \"title\" and populate it with the title from the metadata key called dc:title\n",
    "\n",
    "text_df['title'] = [value.get('dc:title') for value in text_df[\"metadata\"]]\n",
    "\n",
    "## Inspect output as needed\n",
    "text_df.head()\n",
    "\n",
    "## Resources\n",
    "## https://stackoverflow.com/questions/44218812/pandas-add-columns-to-a-dataframe-based-in-dict-from-one-of-the-columns\n",
    "\n",
    "## If needed, you can review the contents of the metadata column using this code:\n",
    "## print(text_df['metadata'])\n",
    "## my_dict.keys()[0]     -> key of \"first\" element\n",
    "## my_dict.values()[0]   -> value of \"first\" element\n",
    "## my_dict.items()[0]    -> (key, value) tuple of \"first\" element\n",
    "##list(contacts.items())[0]\n",
    "\n",
    "# for v in text_df['metadata']:\n",
    "#     new = list(v.items())[1]\n",
    "#     print(new)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new column in the dataframe called \"file_name\" and populate it with the title from the metadata key called \"resourceName\"\n",
    "\n",
    "text_df['file_name'] = [value.get('resourceName') for value in text_df[\"metadata\"]]\n",
    "\n",
    "## Inspect output as needed\n",
    "\n",
    "text_df.head()\n",
    "\n",
    "## Resources\n",
    "## https://stackoverflow.com/questions/44218812/pandas-add-columns-to-a-dataframe-based-in-dict-from-one-of-the-columns\n",
    "\n",
    "## If needed, you can review the contents of the metadata column using this code:\n",
    "## print(text_df['metadata'])\n",
    "## my_dict.keys()[0]     -> key of \"first\" element\n",
    "## my_dict.values()[0]   -> value of \"first\" element\n",
    "## my_dict.items()[0]    -> (key, value) tuple of \"first\" element\n",
    "##list(contacts.items())[0]\n",
    "\n",
    "# for v in text_df['metadata']:\n",
    "#     new = list(v.items())[1]\n",
    "#     print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change the file type of the file_name column to string (if it isn't already). \n",
    "\n",
    "text_df['file_name'] = text_df['file_name'].astype(str)\n",
    "\n",
    "## Inspect output as needed\n",
    "text_df.head()\n",
    "print(type(text_df['file_name'][1]))\n",
    "print(text_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the b' before the string name \n",
    "text_df['file_name'] = text_df['file_name'].str.strip(\"b\\'\\\"\")\n",
    "\n",
    "## Inspect output as needed\n",
    "text_df.head()\n",
    "\n",
    "## Other method\n",
    "## text_df['file_name'] = text_df['file_name'].str.decode('utf-8')\n",
    "## text_df.head()\n",
    "\n",
    "## Resources\n",
    "## https://stackoverflow.com/questions/46696679/removing-b-from-string-column-in-a-pandas-dataframe\n",
    "## https://stackoverflow.com/questions/61970212/pandas-decoding-a-string-returns-nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count the number of rows in the dataframe\n",
    "\n",
    "text_df_count_row = text_df.shape[0]  # gives number of row count\n",
    "print(text_df_count_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code was written to inspect the \"file_name\" column, but is no longer needed for this workflow. \n",
    "## Sort the dataframe by values in the \"file_name\" column\n",
    "## Notice that some file_names have .pdf or other file type included...this will cause problems\n",
    "## text_df = text_df.sort_values(by=['file_name'], ascending=False)\n",
    "## text_df.head()\n",
    "## Resources\n",
    "## https://stackoverflow.com/questions/37787698/how-to-sort-pandas-dataframe-from-one-column\n",
    "\n",
    "## Inspect the output as needed\n",
    "# print(text_df['file_name'][0])\n",
    "# print(text_df['file_name'][1])\n",
    "# print(text_df['file_name'][2])\n",
    "# print(text_df['file_name'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening data: if data is a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check working directory\n",
    "# cwd = os.getcwd()\n",
    "# print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## path_to_protocol5 = \"C:\\\\Users\\\\keg827\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\Lib\\\\site-packages\\\\pickle5\"\n",
    "## https://www.geeksforgeeks.org/dataframe-read_pickle-method-in-pandas/\n",
    "# with open(path_to_protocol5, \"rb\") as fh:\n",
    "# altmetric_policy_df = pd.read_pickle(r'C:\\Users\\keg827\\Documents\\10_Github_Repos\\CARDIAimpactanalysis\\data\\policy_fulltext (1).pkl', compression=None)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check recursion depth\n",
    "## https://riptutorial.com/python/example/17855/increasing-the-maximum-recursion-depth\n",
    "# def cursing(depth):\n",
    "#   try:\n",
    "#     cursing(depth + 1) # actually, re-cursing\n",
    "#   except RuntimeError as RE:\n",
    "#     print('I recursed {} times!'.format(depth))\n",
    "# cursing(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.getrecursionlimit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.setrecursionlimit(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = \"C:\\\\Users\\\\keg827\\\\Documents\\\\10_Github_Repos\\\\CARDIAimpactanalysis\\\\data\\\\policy_fulltext (1).pkl\"\n",
    "# infile = open(filename,'rb')\n",
    "# new_dict = pickle.load(infile)\n",
    "# infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(type(new_dict))\n",
    "# new_dict.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload Altmetric data\n",
    "## pmid_list = pd.read_csv(r\"C:\\Users\\keg827\\Documents\\10_Github_Repos\\pubmedbiopython\\cardiaids.csv\", encoding= 'unicode_escape')\n",
    "# altmetric_policy_df = pd.read_csv(r\"C:\\Users\\keg827\\Documents\\10_Github_Repos\\CARDIAimpactanalysis\\data\\policy_fulltext.csv\")\n",
    "# altmetric_policy_df.head()\n",
    "\n",
    "## print(altmetric_policy_df['metadata'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Creation Date from Metadata column (if it exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new column called metadata2 which is a reformatted copy of the metadata column from string to object\n",
    "\n",
    "# text_df['metadata2']=text_df['metadata'].apply(ast.literal_eval)\n",
    "\n",
    "## Resource(s)\n",
    "## https://stackoverflow.com/questions/56102724/how-to-convert-string-representation-of-dictionary-in-pandas-dataframe-to-a-new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the creation-date data from the metadata2 column and create a new column called date\n",
    "\n",
    "# text_df['date'] = [value.get('Creation-Date') for value in text_df[\"metadata2\"]]\n",
    "# text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new column called year and extract the year from the date column\n",
    "\n",
    "## efficient way to extract year from string format date\n",
    "# text_df['year'] = pd.DatetimeIndex(text_df['date']).year\n",
    "# text_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Creation Date from outside CSV (if not in PDF metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dataframe from non-core spreadsheet with year column\n",
    "# date_df = pd.read_csv(\"data/10_News_All_Final_2.csv\", encoding='ISO-8859-1') \n",
    "# date_df.head()\n",
    "## print(date_df['File_Name'][4])\n",
    "## date_df.keys()\n",
    "\n",
    "## print(type(date_df['File_Name'][1]))\n",
    "## print(date_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count the number of rows in the dataframe\n",
    "\n",
    "# date_df_count_row = date_df.shape[0]  # gives number of row count\n",
    "# print(date_df_count_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Match text_df and date_df based on title and Metadata_title to add year to the text_df dataframe\n",
    "\n",
    "# merged_df= text_df.merge(date_df, left_on = 'title', right_on = 'Metadata_Title', how = 'inner')\n",
    "# merged_df.keys()\n",
    "# merged_df.head()\n",
    "\n",
    "## Left Dataframe is text_df\n",
    "## Right Dataframe is date_df\n",
    "\n",
    "## Note: An inner join only merges together what the two spreadsheets have in common. Anything not in common will be dropped. \n",
    "\n",
    "## Resources\n",
    "## https://stackoverflow.com/questions/49890305/match-two-columns-from-two-dataframes-and-add-items-from-a-third-column-if-cells\n",
    "# miscset = miscset.merge(oset, left_on='subset', right_on='some_items', \n",
    "#     how='inner').drop(columns='some_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CANNOT USE THIS because file_name and File_Name are not matchable in this dataset\n",
    "## Match text_df and date_df based on file_name and File_Name to add year to the text_df dataframe\n",
    "\n",
    "# merged_df= text_df.merge(date_df, left_on = 'file_name', right_on = 'File_Name', how = 'inner')\n",
    "# merged_df.keys()\n",
    "# merged_df.head()\n",
    "\n",
    "## Left Dataframe is text_df\n",
    "## Right Dataframe is date_df\n",
    "\n",
    "## Note: An inner join only merges together what the two spreadsheets have in common. Anything not in common will be dropped. \n",
    "\n",
    "## Resources\n",
    "## https://stackoverflow.com/questions/49890305/match-two-columns-from-two-dataframes-and-add-items-from-a-third-column-if-cells\n",
    "# miscset = miscset.merge(oset, left_on='subset', right_on='some_items', \n",
    "#     how='inner').drop(columns='some_items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count the number of rows in the dataframe\n",
    "\n",
    "# merged_df_count_row = merged_df.shape[0]  # gives number of row count\n",
    "# print(merged_df_count_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify and remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify any duplicates in the dataframe by Metadata_Title, Source_title, and Year\n",
    "\n",
    "# duplicate_df = merged_df[merged_df.duplicated(subset=['Metadata_Title','Source_title ', 'Year'], keep=False)]\n",
    "# duplicate_df.head()\n",
    "\n",
    "## Count the number of rows in the dataframe\n",
    "\n",
    "# duplicate_df_count_row = duplicate_df.shape[0]  # gives number of row count\n",
    "# print(duplicate_df_count_row)\n",
    "\n",
    "## Note that some files may have multiple duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop duplicates from the merged_df using the \"Metadata_Title\" AND \"Source_title\" and 'Year'...and keep the \"last record\" \n",
    "\n",
    "# deduplicate_df = merged_df.drop_duplicates(['Metadata_Title','Source_title ', 'Year'], keep= 'last')\n",
    "# print(deduplicate_df)\n",
    "\n",
    "## Count the number of rows in the dataframe\n",
    "\n",
    "# deduplicate_df_count_row = deduplicate_df.shape[0]  # gives number of row count\n",
    "# print(deduplicate_df_count_row)\n",
    "\n",
    "## Resources\n",
    "## https://www.geeksforgeeks.org/python-pandas-dataframe-drop_duplicates/\n",
    "## https://www.codegrepper.com/code-examples/python/find+duplicated+rows+with+respect+to+multiple+columns+pandas\n",
    "## https://stackoverflow.com/questions/32093829/remove-duplicates-from-dataframe-based-on-two-columns-a-b-keeping-row-with-max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count the number of duplicates removed from the dataframe\n",
    "\n",
    "# duplicates_removed = merged_df_count_row - deduplicate_df_count_row\n",
    "# print(duplicates_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find any missing data in the dataframe\n",
    "## If any column is mising data, the number of rows with missing data will be reported in this series\n",
    "\n",
    "# deduplicate_df.isnull().sum(axis = 0)\n",
    "\n",
    "## Resources: \n",
    "## https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe\n",
    "## https://stackoverflow.com/questions/46864740/selecting-a-subset-using-dropna-to-select-multiple-columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop rows where values in these columns are NaN\n",
    "\n",
    "# first_drop_df = deduplicate_df.dropna(subset=['content'], how = 'all')\n",
    "# text_content_df = first_drop_df.dropna(subset=['Year'], how = 'all')\n",
    "# text_content_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count the number of rows in the dataframe\n",
    "\n",
    "# text_content_df_count_row = text_content_df.shape[0]  # gives number of row count\n",
    "# print(text_content_df_count_row)\n",
    "\n",
    "## Find any missing data in the dataframe\n",
    "## If any column is mising data, the number of rows with missing data will be reported in this series\n",
    "\n",
    "# text_content_df.isnull().sum(axis = 0)\n",
    "\n",
    "## Resources: \n",
    "## https://stackoverflow.com/questions/15943769/how-do-i-get-the-row-count-of-a-pandas-dataframe\n",
    "## https://stackoverflow.com/questions/46864740/selecting-a-subset-using-dropna-to-select-multiple-columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure to sort the dataframe by YEAR and reset the index. \n",
    "## This is VERY important for using the Dynamic Topic Modeling later on. \n",
    "\n",
    "# final_df = text_content_df.sort_values(['Year'], ascending=True).reset_index(drop=True)\n",
    "# final_df.head(15)\n",
    "\n",
    "## Resources\n",
    "## https://stackoverflow.com/questions/53332116/reset-index-after-sorting-data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a unique identifier to each document in the dataset\n",
    "\n",
    "## If following the remove duplicates workflow, use this code:\n",
    "# final_df['unique_id']=final_df.index\n",
    "# final_df.head(15)\n",
    "\n",
    "## If you did not use the \"remove duplicates\" workflow, use this code:\n",
    "\n",
    "text_df['unique_id']=text_df.index\n",
    "final_df = text_df\n",
    "final_df.head(15)\n",
    "\n",
    "## Resources\n",
    "## https://stackoverflow.com/questions/44878740/how-do-i-create-a-unique-record-id-in-a-python-dataframe\n",
    "## df['unique_id'] = df.longstrings.map(hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove \"new line\" separater (usually 'r\\n\\') so as to not create a content column where the text breaks into new rows.\n",
    "## You will notice that some lines do break in Excel, but this is a problem with excel and not with the CSV. \n",
    "\n",
    "final_df = final_df.replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\r\\n',  ' ', regex=True)\n",
    "final_df.head(15)\n",
    "\n",
    "## Resource(s)\n",
    "## https://stackoverflow.com/questions/46522652/replacing-newlines-with-spaces-for-str-columns-through-pandas-dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save dataframe to csv\n",
    "with open(r\"output/loading/final_df.csv\", 'w', encoding='utf-8') as file:\n",
    "    final_df.to_csv(file, line_terminator='\\n', index=True)\n",
    "    ##  \n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a list of the unique IDS\n",
    "identifiers = final_df['unique_id'].tolist()\n",
    "print(identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the list using pkl\n",
    "\n",
    "file_name = \"output/loading/identifiers.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(identifiers, open_file, protocol=4)\n",
    "open_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
