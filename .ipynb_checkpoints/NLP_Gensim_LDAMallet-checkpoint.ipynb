{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP and Topic Modeling with Gensim LDAMallet\n",
    "\n",
    "Prior to running this code, complete the these notebooks: \n",
    "* NLP_Data_Loading\n",
    "* NLP_Data_Preprocessing\n",
    "* NLP_Genism_Bag_of_Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keg827\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\backend_bases.py:55: DeprecationWarning: PILLOW_VERSION is deprecated and will be removed in a future release. Use __version__ instead.\n",
      "  from PIL import PILLOW_VERSION\n",
      "C:\\Users\\keg827\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\matplotlib\\backend_bases.py:55: DeprecationWarning: PILLOW_VERSION is deprecated and will be removed in a future release. Use __version__ instead.\n",
      "  from PIL import PILLOW_VERSION\n",
      "C:\\Users\\keg827\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "## General Dependencies\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import sys, os\n",
    "import glob\n",
    "from tika import parser # pip install tika\n",
    "import inspect\n",
    "import datetime\n",
    "import pickle5 as pickle\n",
    "\n",
    "## Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import models\n",
    "#from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models import ldaseqmodel\n",
    "\n",
    "\n",
    "## Preprocessing\n",
    "import spacy\n",
    "import nltk as nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "## Plotting\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import ast\n",
    "\n",
    "## Other Libraries\n",
    "from operator import itemgetter\n",
    "\n",
    "## ScikitLearn\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data csv as a dataframe\n",
    "final_df = pd.read_csv(\"output/loading/final_df.csv\", index_col=0) \n",
    "\n",
    "## Inspect output as needed\n",
    "## final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data csv as a dataframe\n",
    "test_df = pd.read_csv(\"output/processing/nostop_ngrams_partsofspeech_nameentities_lemmatize.csv\", index_col=0) \n",
    "\n",
    "## Inspect output as needed\n",
    "## test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open text_out_2 pickle file\n",
    "\n",
    "file_name = \"output/processing/texts_out_2.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"rb\")\n",
    "texts_out_2 = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open bow_corpus pickle file\n",
    "\n",
    "file_name = \"output/bow/bow_corpus.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"rb\")\n",
    "bow_corpus = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open dictionary pickle file\n",
    "\n",
    "file_name = \"output/bow/dictionary.pkl\"\n",
    "\n",
    "dictionary = corpora.Dictionary.load(file_name)\n",
    "\n",
    "# open_file = open(file_name, \"rb\")\n",
    "# dictionary = pickle.load(open_file)\n",
    "# open_file.close()\n",
    "\n",
    "## Resources\n",
    "## https://stackoverflow.com/questions/58961983/how-do-you-save-a-model-dictionary-and-corpus-to-disk-in-gensim-and-then-load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open id_words_count pickle file\n",
    "\n",
    "file_name = \"output/bow/id_words_count.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"rb\")\n",
    "id_words_count = pickle.load(open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Gensim LDAMallet wrapper (instead of the LDAModel)\n",
    "\n",
    "\"Mallet, an open source toolkit, was written by Andrew McCullum. It is basically a Java based package which is used for NLP, document classification, clustering, topic modeling, and many other machine learning applications to text. It provides us the Mallet Topic Modeling toolkit which contains efficient, sampling-based implementations of LDA as well as Hierarchical LDA.\"\n",
    "\n",
    "The Gensim LDAMallet may provide better quality of topics than the Gensim LDAModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\keg827\\Tools\\mallet-2.0.8\\bin\\mallet\n"
     ]
    }
   ],
   "source": [
    "## Provide a Path to Mallet File\n",
    "os.environ.update({'MALLET_HOME':r'C:\\Users\\keg827\\Tools\\mallet-2.0.8'}) \n",
    "#You should update this path as per the path of Mallet directory on your system.\n",
    "mallet_path = r'C:\\Users\\keg827\\Tools\\mallet-2.0.8\\bin\\mallet' \n",
    "#You should update this path as per the path of Mallet directory on your system.\n",
    "print(mallet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic code for LDAMallet if optimal number of topics is already known\n",
    "# ldamallet = gensim.models.wrappers.LdaMallet(\n",
    "#    mallet_path, corpus=bow_corpus, num_topics=6, id2word=dictionary\n",
    "# )\n",
    "# pprint(ldamallet.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find optimal number of topics using the Coherence Score in LDAMallet\n",
    "\n",
    "\"My approach to finding the optimal number of topics is to build many [LDAMallet] models with different values of number of topics (k) and pick the one that gives the highest coherence value.Choosing a ‘k’ that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics.If you see the same keywords being repeated in multiple topics, it’s probably a sign that the ‘k’ is too large.\"\n",
    "\n",
    "The compute_coherence_values() (see below) trains multiple [LDAMallet] models and provides the models and their corresponding coherence scores.\n",
    "\n",
    "Text from: <https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#17howtofindtheoptimalnumberoftopicsforlda>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the optimal number of topics by running the LDAMallet model on multiple numbers of topics, here we are using topics 1 - 20\n",
    "## The compute_coherence_values() (see below) trains multiple LDAMallet models and provides the models and their corresponding coherence scores.\n",
    "## This step will take a lot longer to run than one would expect. \n",
    "## LDAMallet does not implement perplexity scores: https://stackoverflow.com/questions/55278701/gensim-topic-modeling-with-mallet-perplexity\n",
    "\n",
    "\n",
    "def coherence_values_computation_LDAMallet(dictionary, corpus, texts, limit, start, step):\n",
    "    coherence_values_LDAMallet = []\n",
    "    model_list_LDAMallet = []\n",
    "    \n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list_LDAMallet.append(model)\n",
    "        \n",
    "               \n",
    "        ## Compute Coherence Value\n",
    "        ## The LDA model (model) we created above can be used to compute the model’s coherence score \n",
    "        ## i.e. the average /median of the pairwise word-similarity scores of the words in the topic. \n",
    "        coherencemodel_LDAMallet = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values_LDAMallet.append(coherencemodel_LDAMallet.get_coherence())\n",
    "    \n",
    "    return model_list_LDAMallet, coherence_values_LDAMallet\n",
    "\n",
    "model_list_LDAMallet, coherence_values_LDAMallet = coherence_values_computation_LDAMallet (dictionary=dictionary, \n",
    "                                                                                           corpus=bow_corpus, \n",
    "                                                                                           texts=texts_out_2, \n",
    "                                                                                           start=1, \n",
    "                                                                                           limit=20, \n",
    "                                                                                           step=1)\n",
    "## Inspect output as needed \n",
    "print(coherence_values_LDAMallet)\n",
    "print(model_list_LDAMallet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize the coherence scores for each LDAMallet for topics 1 - 20\n",
    "\n",
    "limit=20 \n",
    "start=1 \n",
    "step=1\n",
    "x = range(start, limit, step)\n",
    "print(x)\n",
    "print(coherence_values_LDAMallet)\n",
    "plt.plot(x, coherence_values_LDAMallet)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "\n",
    "# save the figure\n",
    "plt.savefig('output/lda_mallet/LDAMallet_coherence_scores.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print and save the Coherence Scores and Perplexity value\n",
    "\n",
    "## Using zip() to map values \n",
    "coherence_list_LDAMallet = list(zip(x, coherence_values_LDAMallet))\n",
    "## print(type(coherence_list_LDAMallet))\n",
    "\n",
    "## Create a pandas dataframe from list\n",
    "LDAMallet_coherence_df = pd.DataFrame(coherence_list_LDAMallet, columns = ['Number_of_Topics', 'Coherence_Value']) \n",
    "\n",
    "## Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values_LDAMallet):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))\n",
    "    \n",
    "## Resources\n",
    "## https://www.geeksforgeeks.org/zip-in-python/\n",
    "## https://www.geeksforgeeks.org/create-pandas-dataframe-from-lists-using-zip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save dataframe to csv\n",
    "with open(r\"output/lda_mallet/LDAMallet_coherence_score_values.csv\", 'w', encoding='utf-8') as file:\n",
    "    LDAMallet_coherence_df.to_csv(file, index=False, line_terminator='\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the coherence score seems to keep increasing, it may make better sense to pick the model that gave the highest CV before flattening out. \n",
    "<https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#17howtofindtheoptimalnumberoftopicsforlda>\n",
    "\n",
    "Now, the question arises which model should we pick now? One of the good practices is to pick the model, that is giving highest coherence value before flattering out. So that’s why, we will be choosing the model with 25 topics which is at number 4 in the above list.\n",
    "<https://www.tutorialspoint.com/gensim/gensim_documents_and_lda_model.htm>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add the number in the list of the optimal topic number, then print out that number of topics. \n",
    "optimal_model_LDAMallet = model_list_LDAMallet[5]\n",
    "\n",
    "## Inspect the output as needed\n",
    "# print(optimal_model_LDAMallet)\n",
    "# print(type(optimal_model_LDAMallet))\n",
    "\n",
    "## Use \"show_topics()\" to create a list of topics from the genism LDAMallet object\n",
    "model_topics_LDAMallet = optimal_model_LDAMallet.show_topics(formatted=False, num_words=15)\n",
    "\n",
    "## Inspect the output as needed\n",
    "# pprint(optimal_model_LDAMallet.print_topics(num_words=20))\n",
    "# print(type(model_topics_LDAMallet))\n",
    "# print(model_topics_LDAMallet)\n",
    "\n",
    "## Save the model\n",
    "optimal_model_LDAMallet.save('output/lda_mallet/optimal_LDAMallet_model.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create dataframe for topics\n",
    "df_topics_LDAMallet = pd.DataFrame(model_topics_LDAMallet, columns = ['TopicNum', 'Terms'])\n",
    "#df_topics.head()\n",
    "\n",
    "## Explode the list tuples with Term/Probability into rows\n",
    "Term = []\n",
    "Probability = []\n",
    "TopicNumber = []\n",
    "\n",
    "def create_lists(row):\n",
    "    tuples = row['Terms']\n",
    "    topic = row['TopicNum']\n",
    "    for t in tuples:\n",
    "        Term.append(t[0])\n",
    "        Probability.append(t[1])\n",
    "        TopicNumber.append(topic)\n",
    "\n",
    "df_topics_LDAMallet.apply(create_lists, axis=1)\n",
    "\n",
    "df_topics_final_LDAMallet = pd.DataFrame({\"Term\" : Term, \"Probability\": Probability, \"Topic_Number\": TopicNumber})[[\"Topic_Number\",\"Term\", \"Probability\"]]\n",
    "df_topics_final_LDAMallet.head(50)\n",
    "## https://stackoverflow.com/questions/44758596/split-a-list-of-tuples-in-a-column-of-dataframe-to-columns-of-a-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save topics to CSV\n",
    "with open(r\"output/lda_mallet/LDAMallet_topics.csv\", 'w', encoding='utf-8') as file:\n",
    "    df_topics_final_LDAMallet.to_csv(file, index=False, line_terminator='\\n')\n",
    "    file.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding dominant topics in sentences for LDAMallet\n",
    "\n",
    "Finding dominant topics in sentences is one of the most useful practical applications of topic modeling. It determines what topic a given document is about. Here, we will find that topic number which has the highest percentage contribution in that particular document. In order to aggregate the information in a table, we will be creating a function named dominant_topics() \n",
    "\n",
    "Text from: \n",
    "* <https://www.tutorialspoint.com/gensim/gensim_documents_and_lda_model.htm>\n",
    "\n",
    "Resources:\n",
    "* <https://stackoverflow.com/questions/44571617/probabilities-returned-by-gensims-get-document-topics-method-doesnt-add-up-to>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences_LDAMallet(ldamodel=optimal_model_LDAMallet, corpus=bow_corpus, texts=texts_out_2):\n",
    "    ## Init output\n",
    "    sent_topics_df_LDAMallet = pd.DataFrame()\n",
    "\n",
    "    ## Next, we will get the main topics in every document −\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        ## print(i)\n",
    "        ## print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        \n",
    "        ## Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df_LDAMallet = sent_topics_df_LDAMallet.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df_LDAMallet.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df_LDAMallet = pd.concat([sent_topics_df_LDAMallet, contents], axis=1)\n",
    "    \n",
    "    return(sent_topics_df_LDAMallet)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords_LDAMallet = format_topics_sentences_LDAMallet(ldamodel=optimal_model_LDAMallet, corpus=bow_corpus, texts=texts_out_2)\n",
    "\n",
    "## Format dataframe\n",
    "df_dominant_topic_LDAMallet = df_topic_sents_keywords_LDAMallet.reset_index()\n",
    "df_dominant_topic_LDAMallet.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "## Merge with final_df to get document information\n",
    "dominant_topic_final_LDAMallet = df_dominant_topic_LDAMallet.merge(final_df, left_on = 'Document_No', right_index=True).drop(columns= ['title',\n",
    "                                                                                                                            'metadata', \n",
    "                                                                                                                            'content', \n",
    "                                                                                                                            'status',\n",
    "                                                                                                                            'file_name'\n",
    "                                                                                                                            #'preprocess',\n",
    "                                                                                                                            #'tokens',\n",
    "                                                                                                                            #'no_stop',\n",
    "                                                                                                                            #'bigrams',\n",
    "                                                                                                                            #'trigrams',\n",
    "                                                                                                                            #'parts_of_speech',\n",
    "                                                                                                                            #'lemmatize'\n",
    "                                                                                                                             ])\n",
    "\n",
    "## Show dataframe\n",
    "dominant_topic_final_LDAMallet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save dataframe to csv\n",
    "with open(r\"output/lda_mallet/LDAMallet_dominant_topics.csv\", 'w', encoding='utf-8') as file:\n",
    "    dominant_topic_final_LDAMallet.to_csv(file, index=False, line_terminator='\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Most Representative Document in LDAMallet\n",
    "In order to understand more about the topic, we can also find the documents a given topic has contributed to the most. We can infer that topic by reading that particular document(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_dominant_topic_LDAMallet.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Topic_Perc_Contrib'], ascending=[0]).head(5)], \n",
    "                                            axis=0)\n",
    "\n",
    "representative_doc_LDAMallet = sent_topics_sorteddf_mallet.merge(final_df, left_on = 'Document_No', right_index=True).drop(columns= ['title',\n",
    "                                                                                                                            'metadata', \n",
    "                                                                                                                            'content', \n",
    "                                                                                                                            'status',\n",
    "                                                                                                                            'file_name'\n",
    "                                                                                                                            #'preprocess',\n",
    "                                                                                                                            #'tokens',\n",
    "                                                                                                                            #'no_stop',\n",
    "                                                                                                                            #'bigrams',\n",
    "                                                                                                                            #'trigrams',\n",
    "                                                                                                                            #'parts_of_speech',\n",
    "                                                                                                                            #'lemmatize'\n",
    "                                                                                                                             ])\n",
    "\n",
    "representative_doc_LDAMallet.reset_index(drop=True, inplace=True)\n",
    "representative_doc_LDAMallet.head()\n",
    "\n",
    "\n",
    "## Resources\n",
    "## https://stackoverflow.com/questions/31528819/using-merge-on-a-column-and-index-in-pandas\n",
    "## https://stackoverflow.com/questions/17978133/python-pandas-merge-only-certain-columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save dataframe to csv\n",
    "with open(r\"output/lda_mallet/LDAMallet_most_reprentative_document.csv\", 'w', encoding='utf-8') as file:\n",
    "    representative_doc_LDAMallet.to_csv(file, index=False, line_terminator='\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic distribution across documents for LDAMallet\n",
    "Finally, we want to understand the volume and distribution of topics in order to judge how widely it was discussed. The below table exposes that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code needs work (atleast, what is commented out needs work, but what is not commented out is fine)\n",
    "## Number of Documents for Each Topic\n",
    "topic_counts_LDAMallet = df_topic_sents_keywords_LDAMallet['Dominant_Topic'].value_counts()\n",
    "\n",
    "## Percentage of Documents for Each Topic\n",
    "topic_contribution_LDAMallet = round(topic_counts_LDAMallet/topic_counts_LDAMallet.sum(), 4)\n",
    "\n",
    "## Topic Number and Keywords\n",
    "#topic_num_keywords_LDAMallet = df_topic_sents_keywords_LDAMallet[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "## Concatenate Column wise\n",
    "df_topic_distribution_LDAMallet = pd.concat([topic_counts_LDAMallet, topic_contribution_LDAMallet], axis=1)\n",
    "\n",
    "## REMOVED topic_num_keywords_LDAMallet\n",
    "\n",
    "## Change Column names\n",
    "df_topic_distribution_LDAMallet.columns = [ 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "## REMOVED column names: 'Dominant_Topic', 'Topic_Keywords',\n",
    "\n",
    "# Show\n",
    "df_topic_distribution_LDAMallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save dataframe to csv\n",
    "with open(r\"output/lda_mallet/LDAMallet_topic_distribution.csv\", 'w', encoding='utf-8') as file:\n",
    "    df_topic_distribution_LDAMallet.to_csv(file, index=True, line_terminator='\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the word counts and the weights of each keyword\n",
    "\n",
    "\"When it comes to the keywords in the topics, the importance (weights) of the keywords matters. Along with that, how frequently the words have appeared in the documents is also interesting to look.\n",
    "\n",
    "Let’s plot the word counts and the weights of each keyword in the same chart.\n",
    "\n",
    "You want to keep an eye out on the words that occur in multiple topics and the ones whose relative frequency is more than the weight. Often such words turn out to be less important. The chart I’ve drawn below is a result of adding several such words to the stop words list in the beginning and re-running the training process.\"\n",
    "\n",
    "Text from: \n",
    "* <https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/#10.-Word-Counts-of-Topic-Keywords>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "visualize_topics = optimal_model_LDAMallet.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in texts_out_2 for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in visualize_topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "visualize_topics_df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=visualize_topics_df.loc[visualize_topics_df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.bar(x='word', height=\"importance\", data=visualize_topics_df.loc[visualize_topics_df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False)\n",
    "    ax.set_xticklabels(visualize_topics_df.loc[visualize_topics_df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)   \n",
    "\n",
    "# save the figure\n",
    "plt.savefig('output/lda_mallet/LDAMallet_wordcount_weight.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize topics using T-SNE\n",
    "\n",
    "\"One very popular method for visualizing document similarity is to use t-distributed stochastic neighbor embedding, t-SNE. Scikit-learn implements this decomposition method as the sklearn.manifold.TSNE transformer. By decomposing high-dimensional document vectors into 2 dimensions using probability distributions from both the original dimensionality and the decomposed dimensionality, t-SNE is able to effectively cluster similar documents. By decomposing to 2 or 3 dimensions, the documents can be visualized with a scatter plot.\"\n",
    "\n",
    "\n",
    "Text from: <https://www.scikit-yb.org/en/latest/api/text/tsne.html>\n",
    "\n",
    "Resources\n",
    "* <https://www.datacamp.com/community/tutorials/introduction-t-sne>\n",
    "* <https://www.programmersought.com/article/90321584021/>\n",
    "\n",
    "\n",
    "Scikit-learn has an implementation of t-SNE available, and you can check its documentation here. It provides a wide variety of tuning parameters for t-SNE, and the most notable ones are:\n",
    "\n",
    "* n_components (default: 2): Dimension of the embedded space.\n",
    "* perplexity (default: 30): The perplexity is related to the number of nearest neighbors that are used in other manifold learning algorithms. Consider selecting a value between 5 and 50.\n",
    "* early_exaggeration (default: 12.0): Controls how tight natural clusters in the original space are in the embedded space and how much space will be between them.\n",
    "* learning_rate (default: 200.0): The learning rate for t-SNE is usually in the range (10.0, 1000.0).\n",
    "* n_iter (default: 1000): Maximum number of iterations for the optimization. Should be at least 250.\n",
    "* method (default: ‘barnes_hut’): Barnes-Hut approximation runs in O(NlogN) time. method=’exact’ will run on the slower, but exact, algorithm in O(N^2) time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic weights and dominant topics ------------\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.plotting import figure, output_file, show\n",
    "from bokeh.models import Label\n",
    "from bokeh.io import output_notebook\n",
    "\n",
    "print(type(optimal_model_LDAMallet))\n",
    "print(type(bow_corpus))\n",
    "\n",
    "# Get topic weights\n",
    "topic_weights = []\n",
    "\n",
    "## Next, we will get the main topics and their frequencies (i.e. weights) for each document\n",
    "for i, row_list in enumerate(optimal_model_LDAMallet[bow_corpus]):\n",
    "#     print(i)\n",
    "#     print(type(i))\n",
    "#    print(row_list)\n",
    "#     print(type(row_list))\n",
    "    topic_weights.append([w for i, w in row_list])\n",
    "    ## w is the output expression\n",
    "    ## for i,w in row_list; which is: i is the topic number, w is the weight\n",
    "    \n",
    "\n",
    "\n",
    "#print(\"*********************This is topic_weights:\", topic_weights)\n",
    "\n",
    "## Array of topic weights    \n",
    "arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "print(\"*********************This is arr:\", arr[0])\n",
    "print(type(arr))\n",
    "\n",
    "## Add original text to the end of the output\n",
    "contents = pd.Series(texts_out_2)\n",
    "# print(\"*********************This is contents:\", contents)\n",
    "#arr = pd.concat([arr, contents], axis=1)\n",
    "\n",
    "## Keep the well separated points (optional)\n",
    "##arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "\n",
    "# Dominant topic number in each doc\n",
    "topic_num = np.argmax(arr, axis=1)\n",
    "\n",
    "# tSNE Dimension Reduction\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "tsne_lda = tsne_model.fit_transform(arr)\n",
    "print(type(tsne_lda))\n",
    "print(tsne_lda)\n",
    "\n",
    "# Plot the Topic Clusters using Bokeh\n",
    "output_notebook()\n",
    "n_topics = 6\n",
    "mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "              plot_width=900, plot_height=700)\n",
    "plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "show(plot)\n",
    "\n",
    "\n",
    "## Resources: \n",
    "## https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/#13.-t-SNE-Clustering-Chart\n",
    "## https://stackoverflow.com/questions/2552287/python-enumerate-built-in-error-when-using-the-start-parameter\n",
    "## https://distill.pub/2016/misread-tsne/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tethne for Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tethne is not working...but here are my attempts to make it work. \n",
    "\n",
    "## from tethne.model.corpus import mallet\n",
    "## pip3 install tethne --pre\n",
    "## If pip install tethne doesn't work, download an unzip the Tethne file (see linkin Resources) \n",
    "## Save the Tethne zip file into a Tools folder (see path I used below)\n",
    "## tethne_mallet_path = C:\\Users\\keg827\\Tools\\tethne-0.8.1.dev12\n",
    "## Then use anaconda prompt to cd /path/to/unzipped/tethne-x.y.z and type in python setup.py install\n",
    "\n",
    "\n",
    "## Resources\n",
    "## http://diging.github.io/tethne/doc/0.6.1-beta/installation.html\n",
    "## http://diging.github.io/tethne/tutorial.gensim.html#let-tethne-talk-to-gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If ## pip3 install tethne --pre doesn't work then...\n",
    "\n",
    "## Provide a Path to Tethne Mallet File\n",
    "#os.environ.update({'TETHNE_MALLET_HOME':r'C:\\Users\\keg827\\Tools\\tethne-0.8.1.dev12'}) \n",
    "## You should update this path as per the path of Mallet directory on your system.\n",
    "#tethne_mallet_path = r'C:\\Users\\keg827\\Tools\\tethne-0.8.1.dev12\\tethne\\bin\\mallet-2.0.7\\bin\\mallet' \n",
    "## You should update this path as per the path of Mallet directory on your system.\n",
    "#print(tethne_mallet_path)\n",
    "\n",
    "## Resources\n",
    "## http://diging.github.io/tethne/doc/0.6.1-beta/installation.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check bow_corpus and assign to variable called identifiers\n",
    "# identifiers = range(len(bow_corpus))\n",
    "# print(identifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load a potentially pretrained model from disk.\n",
    "# LDAModel = LdaModel.load('lda_checkpoint/gensim_tutorial_topic_model.lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open identifiers pickle file\n",
    "\n",
    "# file_name = \"output/loading/identifiers.pkl\"\n",
    "\n",
    "# open_file = open(file_name, \"rb\")\n",
    "# identifiers = pickle.load(open_file)\n",
    "# open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tethne import gensim_to_theta_featureset\n",
    "# theta = gensim_to_theta_featureset(LDAModel, bow_corpus, identifiers)\n",
    "\n",
    "## Inspect output as needed\n",
    "# print(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save theta\n",
    "## Save the list as a .pkl file\n",
    "\n",
    "# file_name = \"output/lda_mallet/theta.pkl\"\n",
    "\n",
    "# open_file = open(file_name, \"wb\")\n",
    "# pickle.dump(theta, open_file, protocol=4)\n",
    "# open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open theta pickle file\n",
    "\n",
    "# file_name = \"output/lda_mallet/theta.pkl\"\n",
    "\n",
    "# open_file = open(file_name, \"rb\")\n",
    "# theta = pickle.load(open_file)\n",
    "# open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# from tethne import feature_cooccurrence\n",
    "# graph = feature_cooccurrence(theta, dictionary, min_weight=0.05)\n",
    "# print(graph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tethne.writers as wr\n",
    "# ## wr.graph.to_graphml(graph, 'output/lda_mallet/mymodel.gml')\n",
    "# wr.graph.write_graphml(graph, 'output/lda_mallet/mymodel.graphml')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Current errors with Tethne and possible fixes ##############################\n",
    "\n",
    "\n",
    "\n",
    "######## Problems with zip (now izip)##########\n",
    "\n",
    "## Use this code wherever zip is used:\n",
    "\n",
    "# try:\n",
    "#     from itertools import izip\n",
    "# except ImportError: 3.7\n",
    "#     izip = zip\n",
    "\n",
    "## Resource: https://codereview.stackexchange.com/questions/26271/import-izip-for-different-versions-of-python\n",
    "\n",
    "## Example of where zip is used: \n",
    "## tethne-0.8.1.dev12 -> tethne -> plot -> __init__.py\n",
    "## tethne-0.8.1.dev12 -> tethne -> analyze -> corpus\n",
    "## tethne-0.8.1.dev12 -> tethne -> plot -> init.py\n",
    "## tethne-0.8.1.dev12 -> tethne -> classes -> feature.py\n",
    "## tethne-0.8.1.dev12 -> build -> lib -> tethne -> analyze -> corpus.py\n",
    "## In this file you may also need to change zip to izip here: for year, N in izip(years, values):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### Problems with Python 2 vs. 3 ##########\n",
    "\n",
    "### Tethne is built with Python 2, but now we're on Python 3. \n",
    "## Need to add fix for str and range here:\n",
    "## tethne-0.8.1.dev12 -> build -> lib -> tethne -> analyze -> corpus.py\n",
    "## tethne-0.8.1.dev12 -> tethne -> classes -> feature.py\n",
    "## tethne-0.8.1.dev12 -> tethne -> writers -> graph.py\n",
    "## Add this code:  \n",
    "# PYTHON_3 = sys.version_info[0] == 3\n",
    "# if PYTHON_3:\n",
    "#     unicode = str\n",
    "#     xrange = range\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### Problems with cPickle and pickle #########\n",
    "\n",
    "## Example of this pickle problem:\n",
    "## tethne-0.8.1.dev12 -> tethne -> classes -> streaming.py\n",
    "## comment out: import cPickle as pickle\n",
    "## add in: import pickle\n",
    "\n",
    "\n",
    "\n",
    "######## Problems with iter ##################\n",
    "\n",
    "## Make these changes in this file: \n",
    "## tethne-0.8.1.dev12 -> tethne -> writers -> graph.py\n",
    "\n",
    "## Old code listed first, new replacement code listed second\n",
    "\n",
    "## for node,data in G.nodes_iter(data=True):\n",
    "## for node,data in list(G.nodes(data=True)):\n",
    "\n",
    "## for u,v,key,data in G.edges_iter(data=True, keys=True):\n",
    "## for u,v,key,data in list(G.edges(data=True, keys=True)):\n",
    "\n",
    "## for u,v,data in G.edges_iter(data=True):\n",
    "## for u,v,data in list(G.edges(data=True)):\n",
    "\n",
    "## for k,v in n[1].iteritems():\n",
    "## for k,v in n[1].items():\n",
    "\n",
    "## for k,v in e[2].iteritems():\n",
    "## for k,v in e[2].items():\n",
    "            \n",
    "## for key, value in node_attribs.iteritems():\n",
    "## for key, value in node_attribs.items():\n",
    "\n",
    "## for attrib, value in edge[3].iteritems():\n",
    "## for attrib, value in edge[3].items():\n",
    "\n",
    "## for attrib, value in edge[2].iteritems():\n",
    "## for attrib, value in edge[2].items():\n",
    "\n",
    "\n",
    "\n",
    "## Make these changes in this file: \n",
    "## tethne-0.8.1.dev12 -> tethne -> model-> __init__.py \n",
    "\n",
    "## Old code listed first, new replacement code listed second\n",
    "\n",
    "## for key, value in kwargs.iteritems():\n",
    "## for key, value in kwargs.items():\n",
    "\n",
    "## for key, value in kwargs.iteritems():\n",
    "## for key, value in kwargs.items():\n",
    "\n",
    "\n",
    "####### Problems with iter and graph.node ###########\n",
    "\n",
    "## Make these changes in this file: \n",
    "## tethne-0.8.1.dev12 -> tethne -> netowrks -> base.py \n",
    "\n",
    "## Old code listed first, new replacement code listed second\n",
    "\n",
    "## for combo, count in pairs.iteritems():\n",
    "## for combo, count in pairs.items():\n",
    "\n",
    "## for k, attrs in node_attrs.iteritems():\n",
    "## for k, attrs in node_attrs.items():\n",
    "\n",
    "## if k in graph.node:\n",
    "## if k in graph.nodes:\n",
    "\n",
    "## graph.node[k].update(attrs)\n",
    "## graph.nodes[k].update(attrs)\n",
    "            \n",
    "## for paper, feature in featureset.iteritems():\n",
    "## for paper, feature in featureset.items():\n",
    "        \n",
    "## for elem, papers in featureset.with_feature.iteritems():\n",
    "## for elem, papers in featureset.with_feature.items():       \n",
    "        \n",
    "## for combo, features in pairs.iteritems():\n",
    "## for combo, features in pairs.items():        \n",
    "        \n",
    "## graph.node[node][attr] = value\n",
    "## graph.nodes[node][attr] = value\n",
    "        \n",
    "## for paper, feature in featureset.iteritems():\n",
    "## for paper, feature in featureset.items():\n",
    "        \n",
    "        \n",
    "## Resources:\n",
    "## https://stackoverflow.com/questions/58518554/attributeerror-graph-object-has-no-attribute-node        \n",
    "## #https://stackoverflow.com/questions/33734836/graph-object-has-no-attribute-nodes-iter-in-networkx-module-python      \n",
    "        \n",
    "\n",
    "  \n",
    "    \n",
    "\n",
    "######### After all fixes are made ##############\n",
    "\n",
    "## After each fix is made or all fixes are made, do this in anaconda prompt:  python setup.py install\n",
    "\n",
    "\n",
    "\n",
    "######## Functions of Note for my project ##############\n",
    "\n",
    "# def gensim_to_theta_featureset(model, corpus, identifiers):\n",
    "#     theta = FeatureSet()\n",
    "#     for idx, doc in zip(identifiers, bow_corpus):\n",
    "#         theta.add(idx, Feature(model[doc]))\n",
    "#     return theta\n",
    "\n",
    "# ## Call the function as needed\n",
    "# theta = gensim_to_theta_featureset(model=optimal_model_LDAMallet, corpus=bow_corpus, identifiers= identifiers)\n",
    "# print(theta)\n",
    "\n",
    "## The function gensim_to_theta_featureset is in tethne-0.8.1.dev12 -> tethne -> model -> corpus -> gensim lda\n",
    "## To find where functions are, go to tethne-0.8.1.dev12 -> tethne -> build -> tethne -> __init__.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://gist.github.com/quadrismegistus/eb2360026afce4ef4e57872146369091\n",
    "#from gensim.test.utils import common_texts\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# model = Word2Vec(sentences=texts_out_2, window=5, min_count=1, workers=4)\n",
    "# model.save(\"output/word2vec/ldaword2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_words = texts_out_2\n",
    "\n",
    "# # The number of connections we want: either as a factor of the number of words or a set number\n",
    "# num_top_conns = len(my_words) * 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "\n",
    "# Make a list of all word-to-word distances [each as a tuple of (word1,word2,dist)]\n",
    "# dists=[]\n",
    "\n",
    "# ## Method 1 to find distances: use gensim to get the similarity between each word pair\n",
    "# for i1,word1 in enumerate(my_words):\n",
    "#     print(i1)\n",
    "#     print(word1)\n",
    "#     for i2,word2 in enumerate(my_words):\n",
    "#         if i1>=i2: continue\n",
    "#         cosine_similarity = model.similarity(word1,word2)\n",
    "#         cosine_distance = 1 - cosine_similarity\n",
    "#         dist = (word1, word2, cosine_distance)\n",
    "#         dists.append(dist)\n",
    "\n",
    "# ## Or, Method 2 to find distances: use scipy (faster)\n",
    "# from scipy.spatial.distance import pdist,squareform\n",
    "# Matrix = np.array([model[word] for word in my_words])\n",
    "# dist = squareform(pdist(Matrix,'cosine'))\n",
    "# for i1,word1 in enumerate(my_words):\n",
    "#     for i2,word2 in enumerate(my_words):\n",
    "#         if i1>=i2: continue\n",
    "#         cosine_distance = Matrix[i1, i2]\n",
    "#         dist = (word1, word2, cosine_distance)\n",
    "#         dists.append(dist)\n",
    "\n",
    "# ######\n",
    "\n",
    "# # Sort the list by ascending distance\n",
    "# dists.sort(key=lambda _tuple: _tuple[-1])\n",
    "\n",
    "# # Get the top connections\n",
    "# top_conns = dists[:num_top_conns]\n",
    "\n",
    "# # Make a network\n",
    "# import networkx as nx\n",
    "# g = nx.Graph()\n",
    "# for word1,word2,dist in top_conns:\n",
    "#     weight = 1 - dist # cosine similarity makes more sense for edge weight\n",
    "#     g.add_edge(word1, word2, weight=float(weight))\n",
    "\n",
    "# # Write the network\n",
    "# nx.write_graphml(g, 'my-semantic-network.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
