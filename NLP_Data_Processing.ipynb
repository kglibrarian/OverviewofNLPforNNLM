{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic_Modeling_Altmetric_Data_Processing\n",
    "\n",
    "Prior to running this code, complete the these notebooks: \n",
    "* Topic_Modeling_Altmetric_Data_Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General Dependencies\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import sys, os\n",
    "import glob\n",
    "from tika import parser # pip install tika\n",
    "import inspect\n",
    "import datetime\n",
    "import pickle5 as pickle\n",
    "\n",
    "## Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import models\n",
    "#from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models import ldaseqmodel\n",
    "\n",
    "\n",
    "## Preprocessing\n",
    "import spacy\n",
    "import nltk as nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "## Plotting\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import ast\n",
    "\n",
    "## Other Libraries\n",
    "from operator import itemgetter\n",
    "\n",
    "## ScikitLearn\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from previous notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata</th>\n",
       "      <th>content</th>\n",
       "      <th>status</th>\n",
       "      <th>title</th>\n",
       "      <th>file_name</th>\n",
       "      <th>Database</th>\n",
       "      <th>Document_Type</th>\n",
       "      <th>Reference_Detail</th>\n",
       "      <th>CORE</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Title</th>\n",
       "      <th>Year</th>\n",
       "      <th>Publication_Date</th>\n",
       "      <th>Source_title</th>\n",
       "      <th>Page_start</th>\n",
       "      <th>File_Name</th>\n",
       "      <th>Metadata_Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'Content-Type': 'application/pdf', 'Creation-...</td>\n",
       "      <td>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r...</td>\n",
       "      <td>200</td>\n",
       "      <td>UCI MEDICAL CENTER AWARDED $1.3 MILLION FOR NA...</td>\n",
       "      <td>CO.105UCI MEDICAL CENTER AWARDED $1.3 MILLION ...</td>\n",
       "      <td>Nexis Uni</td>\n",
       "      <td>News Newswire</td>\n",
       "      <td>CO</td>\n",
       "      <td>NONCORE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UCI MEDICAL CENTER AWARDED $1.3 MILLION FOR NA...</td>\n",
       "      <td>1990</td>\n",
       "      <td>December 4, 1990, Tuesday</td>\n",
       "      <td>PR Newswire</td>\n",
       "      <td>536 words</td>\n",
       "      <td>CO.105UCI MEDICAL CENTER AWARDED $1.3 MILLION ...</td>\n",
       "      <td>UCI MEDICAL CENTER AWARDED $1.3 MILLION FOR NA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'Content-Type': 'application/pdf', 'Creation-...</td>\n",
       "      <td>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r...</td>\n",
       "      <td>200</td>\n",
       "      <td>KAISER PERMANENTE STUDY: IF YOU'RE OBESE, HOST...</td>\n",
       "      <td>CO.105KAISER PERMANENTE STUDY_ IF YOU'RE OBESE...</td>\n",
       "      <td>Nexis Uni</td>\n",
       "      <td>News Newswire</td>\n",
       "      <td>CO</td>\n",
       "      <td>NONCORE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KAISER PERMANENTE STUDY: IF YOU'RE OBESE, HOST...</td>\n",
       "      <td>1994</td>\n",
       "      <td>March 18, 1994, Friday</td>\n",
       "      <td>PR Newswire</td>\n",
       "      <td>380 words</td>\n",
       "      <td>CO.105KAISER PERMANENTE STUDY_ IF YOU'RE OBESE...</td>\n",
       "      <td>KAISER PERMANENTE STUDY: IF YOU'RE OBESE, HOST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'Content-Type': 'application/pdf', 'Creation-...</td>\n",
       "      <td>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r...</td>\n",
       "      <td>200</td>\n",
       "      <td>YOUNG AMERICAN ADULTS BECOMING FATTER, NOT FIT...</td>\n",
       "      <td>CO.105YOUNG AMERICAN ADULTS BECOMING FATTER, N...</td>\n",
       "      <td>Nexis Uni</td>\n",
       "      <td>News Newswire</td>\n",
       "      <td>CO</td>\n",
       "      <td>NONCORE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>YOUNG AMERICAN ADULTS BECOMING FATTER, NOT FIT...</td>\n",
       "      <td>1995</td>\n",
       "      <td>March 10, 1995, Friday</td>\n",
       "      <td>PR Newswire</td>\n",
       "      <td>749 words</td>\n",
       "      <td>CO.105YOUNG AMERICAN ADULTS BECOMING FATTER, N...</td>\n",
       "      <td>YOUNG AMERICAN ADULTS BECOMING FATTER, NOT FIT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'Content-Type': 'application/pdf', 'Creation-...</td>\n",
       "      <td>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r...</td>\n",
       "      <td>200</td>\n",
       "      <td>New Estimates On The Prevalence Of Hypertrophi...</td>\n",
       "      <td>CO.105New Estimates On The Prevalence Of Hyper...</td>\n",
       "      <td>Nexis Uni</td>\n",
       "      <td>News Newswire</td>\n",
       "      <td>CO</td>\n",
       "      <td>NONCORE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New Estimates On The Prevalence Of Hypertrophi...</td>\n",
       "      <td>1995</td>\n",
       "      <td>August 30, 1995 Wednesday</td>\n",
       "      <td>Reuters Health Medical News</td>\n",
       "      <td>385 words</td>\n",
       "      <td>CO.105New Estimates On The Prevalence Of Hyper...</td>\n",
       "      <td>New Estimates On The Prevalence Of Hypertrophi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'Content-Type': 'application/pdf', 'Creation-...</td>\n",
       "      <td>\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r...</td>\n",
       "      <td>200</td>\n",
       "      <td>KAISER PERMANENTE CORRECTS MISLEADING S.F. CHR...</td>\n",
       "      <td>CO.105KAISER PERMANENTE CORRECTS MISLEADING S....</td>\n",
       "      <td>Nexis Uni</td>\n",
       "      <td>News Newswire</td>\n",
       "      <td>CO</td>\n",
       "      <td>NONCORE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KAISER PERMANENTE CORRECTS MISLEADING S.F. CHR...</td>\n",
       "      <td>1995</td>\n",
       "      <td>November 7, 1995, Tuesday</td>\n",
       "      <td>PR Newswire</td>\n",
       "      <td>1338 words</td>\n",
       "      <td>CO.105KAISER PERMANENTE CORRECTS MISLEADING S....</td>\n",
       "      <td>KAISER PERMANENTE CORRECTS MISLEADING S.F. CHR...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            metadata  \\\n",
       "0  {'Content-Type': 'application/pdf', 'Creation-...   \n",
       "1  {'Content-Type': 'application/pdf', 'Creation-...   \n",
       "2  {'Content-Type': 'application/pdf', 'Creation-...   \n",
       "3  {'Content-Type': 'application/pdf', 'Creation-...   \n",
       "4  {'Content-Type': 'application/pdf', 'Creation-...   \n",
       "\n",
       "                                             content  status  \\\n",
       "0  \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r...     200   \n",
       "1  \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r...     200   \n",
       "2  \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r...     200   \n",
       "3  \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r...     200   \n",
       "4  \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r...     200   \n",
       "\n",
       "                                               title  \\\n",
       "0  UCI MEDICAL CENTER AWARDED $1.3 MILLION FOR NA...   \n",
       "1  KAISER PERMANENTE STUDY: IF YOU'RE OBESE, HOST...   \n",
       "2  YOUNG AMERICAN ADULTS BECOMING FATTER, NOT FIT...   \n",
       "3  New Estimates On The Prevalence Of Hypertrophi...   \n",
       "4  KAISER PERMANENTE CORRECTS MISLEADING S.F. CHR...   \n",
       "\n",
       "                                           file_name   Database  \\\n",
       "0  CO.105UCI MEDICAL CENTER AWARDED $1.3 MILLION ...  Nexis Uni   \n",
       "1  CO.105KAISER PERMANENTE STUDY_ IF YOU'RE OBESE...  Nexis Uni   \n",
       "2  CO.105YOUNG AMERICAN ADULTS BECOMING FATTER, N...  Nexis Uni   \n",
       "3  CO.105New Estimates On The Prevalence Of Hyper...  Nexis Uni   \n",
       "4  CO.105KAISER PERMANENTE CORRECTS MISLEADING S....  Nexis Uni   \n",
       "\n",
       "   Document_Type Reference_Detail     CORE Authors  \\\n",
       "0  News Newswire               CO  NONCORE     NaN   \n",
       "1  News Newswire               CO  NONCORE     NaN   \n",
       "2  News Newswire               CO  NONCORE     NaN   \n",
       "3  News Newswire               CO  NONCORE     NaN   \n",
       "4  News Newswire               CO  NONCORE     NaN   \n",
       "\n",
       "                                              Title   Year  \\\n",
       "0  UCI MEDICAL CENTER AWARDED $1.3 MILLION FOR NA...  1990   \n",
       "1  KAISER PERMANENTE STUDY: IF YOU'RE OBESE, HOST...  1994   \n",
       "2  YOUNG AMERICAN ADULTS BECOMING FATTER, NOT FIT...  1995   \n",
       "3  New Estimates On The Prevalence Of Hypertrophi...  1995   \n",
       "4  KAISER PERMANENTE CORRECTS MISLEADING S.F. CHR...  1995   \n",
       "\n",
       "           Publication_Date                 Source_title   Page_start  \\\n",
       "0  December 4, 1990, Tuesday                  PR Newswire   536 words   \n",
       "1     March 18, 1994, Friday                  PR Newswire   380 words   \n",
       "2     March 10, 1995, Friday                  PR Newswire   749 words   \n",
       "3  August 30, 1995 Wednesday  Reuters Health Medical News   385 words   \n",
       "4  November 7, 1995, Tuesday                  PR Newswire  1338 words   \n",
       "\n",
       "                                           File_Name  \\\n",
       "0  CO.105UCI MEDICAL CENTER AWARDED $1.3 MILLION ...   \n",
       "1  CO.105KAISER PERMANENTE STUDY_ IF YOU'RE OBESE...   \n",
       "2  CO.105YOUNG AMERICAN ADULTS BECOMING FATTER, N...   \n",
       "3  CO.105New Estimates On The Prevalence Of Hyper...   \n",
       "4  CO.105KAISER PERMANENTE CORRECTS MISLEADING S....   \n",
       "\n",
       "                                      Metadata_Title  \n",
       "0  UCI MEDICAL CENTER AWARDED $1.3 MILLION FOR NA...  \n",
       "1  KAISER PERMANENTE STUDY: IF YOU'RE OBESE, HOST...  \n",
       "2  YOUNG AMERICAN ADULTS BECOMING FATTER, NOT FIT...  \n",
       "3  New Estimates On The Prevalence Of Hypertrophi...  \n",
       "4  KAISER PERMANENTE CORRECTS MISLEADING S.F. CHR...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load data csv as a dataframe\n",
    "\n",
    "final_df = pd.read_csv(\"output/loading/final_df.csv\", index_col=0) \n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the text to lower case, remove special characters, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " kaiser permanente study if you re obese hostile or depressed try watching less tv kaiser permanente study if you re obese hostile or depressed try watching less tv pr newswire march friday eastern time  pr newswire association inc section lifestyle length words  a federally funded kaiser permanente study has found a link between watching television and obesity hostility and depression while heavy tv watchers were found to practice poor health habits associated with heart disease such as smoking the study could not determine if tv viewing actually causes heart attacks or heart disease the results of the study were presented today by kaiser permanente physician and lead investigator stephen sidney m d at the th annual conference on cardiovascular disease of epidemiology and prevention in tampa fla funded by the national institutes of health the cardia study examined television viewing and cardiovascular risk factors in young adults in four u s cities a bi racial population of young men and women ages years old in birmingham ala chicago minneapolis and oakland was examined for a wide variety of potential cardiovascular risk factors the study group included black men white men black women and white women the study subjects were questioned about the number of hours of tv they watched per day according to  sidney from the kaiser permanente division of research in oakland those who reported watching tv for four or more hours per day were more than twice as likely to be cigarette smokers and to be physically inactive than who reported watching tv for one hour or less per day these heavy tv viewers were also nearly twice as likely as the lighter viewers to have a high score on a questionnaire measuring hostility were more likely to be obese and more likely to have a high score on a questionnaire measuring depression sidney reported twenty two percent of the study group reported watching four or more hours of television each day tv viewing was inconsistently related to high blood pressure and abnormal blood lipids measurements sidney said tv viewing is potentially modifiable behavior that is significantly associated with obesity adverse habits and adverse psychological measures said sidney however this study could not determine whether tv viewing is causally related to these outcomes kaiser permanente is the nation s largest hmo providing care to million americans  of kaiser permanente study if you re obese hostile or depressed try watching less tv contact beverly hayon of kaiser permanente    subject obesity cardiovascular disease depression diseases disorders health departments heart disease investigations medicine health press releases research institutes research reports smoking women cholesterol epidemiology obesity related diseases science funding psychology conferences conventions hypertension adults company kaiser permanente kaiser permanente kaiser permanente national institutes of health national institutes of health kaiser permanente kaiser permanente national institutes of health national institutes of health organization national institutes of health national institutes of health kaiser permanente national institutes of health national institutes of health kaiser permanente national institutes of health national institutes of health  naics direct health medical insurance carriers sic hospital medical service plans health care hospitalshealth care hospitals health departments television  hospitals epidemiology managed care organizations psychology insurance health maintenance organizations  tampa fl usa birmingham al usa chicago il usa florida usa alabama usa united states  kaiser permanente study if you re obese hostile or depressed try watching less tv   \n"
     ]
    }
   ],
   "source": [
    "## Pre-process the text to lower case, remove special characters, etc. \n",
    "## https://kavita-ganesan.com/extracting-keywords-from-text-tfidf/#.X7RHltBKiUn\n",
    "## Test regex here: https://pythex.org/\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    ## Lowercase words\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    ## Remove Emails from text\n",
    "    ## if you need to match a \\, you can precede them with a backslash to remove their special meaning: \\\\.\n",
    "    ## \\S matches any non-whitespace character; this is equivalent to the class [^ \\t\\n\\r\\f\\v].\n",
    "    ## \\s Matches any whitespace character; this is equivalent to the class [ \\t\\n\\r\\f\\v]\n",
    "    ## Code below matches any character, then an @ sign, then more characters, end matching when a white space is found.\n",
    "    text_email = re.sub('\\\\S*@\\\\S*\\\\s?', '', text_lower) \n",
    "    \n",
    "    ## Remove URLS from text\n",
    "    ## https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python/40823105#40823105\n",
    "    ## text_urls = re.sub(r'http\\S+', '', text_email)\n",
    "    ## https://www.geeksforgeeks.org/python-check-url-string/\n",
    "    text_urls = re.sub(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\",'', text_email)\n",
    "    \n",
    "    \n",
    "    ## Remove tabs and new lines from text\n",
    "    ## https://stackoverflow.com/questions/16355732/how-to-remove-tabs-and-newlines-with-a-regex\n",
    "    ## \\s Matches any whitespace character; this is equivalent to the class [ \\t\\n\\r\\f\\v]\n",
    "    text_spaces = re.sub(r'\\s+',' ',text_urls)\n",
    "        \n",
    "    ## Remove \\n from text\n",
    "    text_space_character = text_spaces.replace('\\n','')\n",
    "    \n",
    "    ## Remove \\t from text\n",
    "    text_tab_character = text_space_character.replace('\\t','')\n",
    "    \n",
    "    ## Remove special characters and numbers\n",
    "    ## \\W matches any non-alphanumeric character; this is equivalent to the class [^a-zA-Z0-9_]\n",
    "    ## \\d matches any decimal digit; this is equivalent to the class [0-9]\n",
    "    text_numbers = re.sub(\"(\\\\d|\\\\W)+\",\" \",text_tab_character)\n",
    "    \n",
    "    ## Remove tags\n",
    "    ##text_tags = re.sub(\"\",\"\",text_numbers)\n",
    "\n",
    "    ## Remove special characters and space, but leave in periods and numbers\n",
    "    ## ^ means any character except. So [^5] will match any character except '5'\n",
    "    ## [^a-zA-Z0-9_] matches any non-alphanumeric character.\n",
    "    ## text_special = re.sub('[^A-Za-z0-9.]+|\\s',' ',text_tab_character)\n",
    "    \n",
    "    ## Remove a sepcial list of terms\n",
    "    ## The prune list is similar to \"stop words\" ... just easier to add/remove words on the fly\n",
    "    ## https://stackoverflow.com/questions/15435726/remove-all-occurrences-of-words-in-a-string-from-a-python-list\n",
    "    \n",
    "    PRUNE_LIST = ['right reserved section',\n",
    "                   'reserved section',\n",
    "                   \"length word byline\", \n",
    "                   \"byline\", \n",
    "                   \"word byline\",\n",
    "                   \"journal code\",\n",
    "                   \"load date\", \n",
    "                   \"english\", \n",
    "                   \"dr\", \n",
    "                   \"publication type magazine\",\n",
    "                   \"type magazine\",\n",
    "                   \"magazine\",\n",
    "                   \"type newspaper\",\n",
    "                   \"publication type newspaper\",\n",
    "                   'newspaper',\n",
    "                   \"group right reserved\",\n",
    "                   'section:',\n",
    "                   'copyright',\n",
    "                   'body',\n",
    "                   'length:',\n",
    "                   'keywords:',\n",
    "                   'introduction',\n",
    "                   'page',\n",
    "                   'methodology',\n",
    "                   'table',\n",
    "                   'discussion',\n",
    "                   'conclusions',\n",
    "                   'references',\n",
    "                   'classification',\n",
    "                   'language',\n",
    "                   'industry',\n",
    "                   'geographic',\n",
    "                   'load-date',\n",
    "                   'end of document',\n",
    "                   'mg dl',\n",
    "                   'mg'\n",
    "                   \n",
    "                  ]\n",
    "\n",
    "    remove = '|'.join(PRUNE_LIST)\n",
    "    regex = re.compile(r'\\b('+remove+r')\\b', flags=re.IGNORECASE)\n",
    "    text_special_remove = regex.sub(\"\", text_numbers)\n",
    "\n",
    "    return text_special_remove\n",
    "\n",
    "## New column \"preprocess\" is formed from applying pre_process function to each item in the \"content\" column in dataframe\n",
    "final_df['preprocess'] = final_df['content'].apply(lambda x:preprocess(x))\n",
    "\n",
    "print(final_df['preprocess'][1])\n",
    "\n",
    "#https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data using Gensim Utils Simple Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kaiser', 'permanente', 'study', 'if', 'you', 're', 'obese', 'hostile', 'or', 'depressed', 'try', 'watching', 'less', 'tv', 'kaiser', 'permanente', 'study', 'if', 'you', 're', 'obese', 'hostile', 'or', 'depressed', 'try', 'watching', 'less', 'tv', 'pr', 'newswire', 'march', 'friday', 'eastern', 'time', 'pr', 'newswire', 'association', 'inc', 'section', 'lifestyle', 'length', 'words', 'federally', 'funded', 'kaiser', 'permanente', 'study', 'has', 'found', 'link', 'between', 'watching', 'television', 'and', 'obesity', 'hostility', 'and', 'depression', 'while', 'heavy', 'tv', 'watchers', 'were', 'found', 'to', 'practice', 'poor', 'health', 'habits', 'associated', 'with', 'heart', 'disease', 'such', 'as', 'smoking', 'the', 'study', 'could', 'not', 'determine', 'if', 'tv', 'viewing', 'actually', 'causes', 'heart', 'attacks', 'or', 'heart', 'disease', 'the', 'results', 'of', 'the', 'study', 'were', 'presented', 'today', 'by', 'kaiser', 'permanente', 'physician', 'and', 'lead', 'investigator', 'stephen', 'sidney', 'at', 'the', 'th', 'annual', 'conference', 'on', 'cardiovascular', 'disease', 'of', 'epidemiology', 'and', 'prevention', 'in', 'tampa', 'fla', 'funded', 'by', 'the', 'national', 'institutes', 'of', 'health', 'the', 'cardia', 'study', 'examined', 'television', 'viewing', 'and', 'cardiovascular', 'risk', 'factors', 'in', 'young', 'adults', 'in', 'four', 'cities', 'bi', 'racial', 'population', 'of', 'young', 'men', 'and', 'women', 'ages', 'years', 'old', 'in', 'birmingham', 'ala', 'chicago', 'minneapolis', 'and', 'oakland', 'was', 'examined', 'for', 'wide', 'variety', 'of', 'potential', 'cardiovascular', 'risk', 'factors', 'the', 'study', 'group', 'included', 'black', 'men', 'white', 'men', 'black', 'women', 'and', 'white', 'women', 'the', 'study', 'subjects', 'were', 'questioned', 'about', 'the', 'number', 'of', 'hours', 'of', 'tv', 'they', 'watched', 'per', 'day', 'according', 'to', 'sidney', 'from', 'the', 'kaiser', 'permanente', 'division', 'of', 'research', 'in', 'oakland', 'those', 'who', 'reported', 'watching', 'tv', 'for', 'four', 'or', 'more', 'hours', 'per', 'day', 'were', 'more', 'than', 'twice', 'as', 'likely', 'to', 'be', 'cigarette', 'smokers', 'and', 'to', 'be', 'physically', 'inactive', 'than', 'who', 'reported', 'watching', 'tv', 'for', 'one', 'hour', 'or', 'less', 'per', 'day', 'these', 'heavy', 'tv', 'viewers', 'were', 'also', 'nearly', 'twice', 'as', 'likely', 'as', 'the', 'lighter', 'viewers', 'to', 'have', 'high', 'score', 'on', 'questionnaire', 'measuring', 'hostility', 'were', 'more', 'likely', 'to', 'be', 'obese', 'and', 'more', 'likely', 'to', 'have', 'high', 'score', 'on', 'questionnaire', 'measuring', 'depression', 'sidney', 'reported', 'twenty', 'two', 'percent', 'of', 'the', 'study', 'group', 'reported', 'watching', 'four', 'or', 'more', 'hours', 'of', 'television', 'each', 'day', 'tv', 'viewing', 'was', 'inconsistently', 'related', 'to', 'high', 'blood', 'pressure', 'and', 'abnormal', 'blood', 'lipids', 'measurements', 'sidney', 'said', 'tv', 'viewing', 'is', 'potentially', 'modifiable', 'behavior', 'that', 'is', 'significantly', 'associated', 'with', 'obesity', 'adverse', 'habits', 'and', 'adverse', 'psychological', 'measures', 'said', 'sidney', 'however', 'this', 'study', 'could', 'not', 'determine', 'whether', 'tv', 'viewing', 'is', 'causally', 'related', 'to', 'these', 'outcomes', 'kaiser', 'permanente', 'is', 'the', 'nation', 'largest', 'hmo', 'providing', 'care', 'to', 'million', 'americans', 'of', 'kaiser', 'permanente', 'study', 'if', 'you', 're', 'obese', 'hostile', 'or', 'depressed', 'try', 'watching', 'less', 'tv', 'contact', 'beverly', 'hayon', 'of', 'kaiser', 'permanente', 'subject', 'obesity', 'cardiovascular', 'disease', 'depression', 'diseases', 'disorders', 'health', 'departments', 'heart', 'disease', 'investigations', 'medicine', 'health', 'press', 'releases', 'research', 'institutes', 'research', 'reports', 'smoking', 'women', 'cholesterol', 'epidemiology', 'obesity', 'related', 'diseases', 'science', 'funding', 'psychology', 'conferences', 'conventions', 'hypertension', 'adults', 'company', 'kaiser', 'permanente', 'kaiser', 'permanente', 'kaiser', 'permanente', 'national', 'institutes', 'of', 'health', 'national', 'institutes', 'of', 'health', 'kaiser', 'permanente', 'kaiser', 'permanente', 'national', 'institutes', 'of', 'health', 'national', 'institutes', 'of', 'health', 'organization', 'national', 'institutes', 'of', 'health', 'national', 'institutes', 'of', 'health', 'kaiser', 'permanente', 'national', 'institutes', 'of', 'health', 'national', 'institutes', 'of', 'health', 'kaiser', 'permanente', 'national', 'institutes', 'of', 'health', 'national', 'institutes', 'of', 'health', 'naics', 'direct', 'health', 'medical', 'insurance', 'carriers', 'sic', 'hospital', 'medical', 'service', 'plans', 'health', 'care', 'hospitalshealth', 'care', 'hospitals', 'health', 'departments', 'television', 'hospitals', 'epidemiology', 'managed', 'care', 'organizations', 'psychology', 'insurance', 'health', 'maintenance', 'organizations', 'tampa', 'fl', 'usa', 'birmingham', 'al', 'usa', 'chicago', 'il', 'usa', 'florida', 'usa', 'alabama', 'usa', 'united', 'states', 'kaiser', 'permanente', 'study', 'if', 'you', 're', 'obese', 'hostile', 'or', 'depressed', 'try', 'watching', 'less', 'tv']\n"
     ]
    }
   ],
   "source": [
    "## Tokenize the data using Gensim Utils Simple Preprocess\n",
    "\n",
    "def tokenize(text):\n",
    "    token_list = gensim.utils.simple_preprocess(str(text), deacc=True)  # deacc=True removes punctuations\n",
    "    return token_list\n",
    "\n",
    "## New column \"tokens\" is formed from applying pre_process function to each item in the \"content\" column in dataframe\n",
    "final_df['tokens'] = final_df['preprocess'].apply(lambda x:tokenize(x))\n",
    "\n",
    "\n",
    "print(final_df['tokens'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords using a custom stopword list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kaiser', 'permanente', 'study', 'obese', 'hostile', 'depressed', 'try', 'watching', 'tv', 'kaiser', 'permanente', 'study', 'obese', 'hostile', 'depressed', 'try', 'watching', 'tv', 'pr', 'newswire', 'march', 'friday', 'eastern', 'time', 'pr', 'newswire', 'association', 'section', 'lifestyle', 'length', 'words', 'federally', 'funded', 'kaiser', 'permanente', 'study', 'found', 'link', 'watching', 'television', 'obesity', 'hostility', 'depression', 'heavy', 'tv', 'watchers', 'found', 'practice', 'poor', 'health', 'habits', 'associated', 'heart', 'disease', 'smoking', 'study', 'determine', 'tv', 'viewing', 'actually', 'causes', 'heart', 'attacks', 'heart', 'disease', 'results', 'study', 'presented', 'today', 'kaiser', 'permanente', 'physician', 'lead', 'investigator', 'stephen', 'sidney', 'th', 'annual', 'conference', 'cardiovascular', 'disease', 'epidemiology', 'prevention', 'tampa', 'fla', 'funded', 'national', 'institutes', 'health', 'cardia', 'study', 'examined', 'television', 'viewing', 'cardiovascular', 'risk', 'factors', 'young', 'adults', 'cities', 'bi', 'racial', 'population', 'young', 'men', 'women', 'ages', 'years', 'old', 'birmingham', 'ala', 'chicago', 'minneapolis', 'oakland', 'examined', 'wide', 'variety', 'potential', 'cardiovascular', 'risk', 'factors', 'study', 'group', 'included', 'black', 'men', 'white', 'men', 'black', 'women', 'white', 'women', 'study', 'subjects', 'questioned', 'number', 'hours', 'tv', 'watched', 'day', 'according', 'sidney', 'kaiser', 'permanente', 'division', 'research', 'oakland', 'who', 'reported', 'watching', 'tv', 'hours', 'day', 'twice', 'likely', 'cigarette', 'smokers', 'physically', 'inactive', 'who', 'reported', 'watching', 'tv', 'hour', 'day', 'heavy', 'tv', 'viewers', 'nearly', 'twice', 'likely', 'lighter', 'viewers', 'high', 'score', 'questionnaire', 'measuring', 'hostility', 'likely', 'obese', 'likely', 'high', 'score', 'questionnaire', 'measuring', 'depression', 'sidney', 'reported', 'twenty', 'percent', 'study', 'group', 'reported', 'watching', 'hours', 'television', 'day', 'tv', 'viewing', 'inconsistently', 'high', 'blood', 'pressure', 'abnormal', 'blood', 'lipids', 'measurements', 'sidney', 'tv', 'viewing', 'potentially', 'modifiable', 'behavior', 'significantly', 'associated', 'obesity', 'adverse', 'habits', 'adverse', 'psychological', 'measures', 'sidney', 'study', 'determine', 'tv', 'viewing', 'causally', 'outcomes', 'kaiser', 'permanente', 'nation', 'largest', 'hmo', 'care', 'million', 'americans', 'kaiser', 'permanente', 'study', 'obese', 'hostile', 'depressed', 'try', 'watching', 'tv', 'contact', 'beverly', 'hayon', 'kaiser', 'permanente', 'subject', 'obesity', 'cardiovascular', 'disease', 'depression', 'diseases', 'disorders', 'health', 'departments', 'heart', 'disease', 'investigations', 'medicine', 'health', 'press', 'releases', 'research', 'institutes', 'research', 'reports', 'smoking', 'women', 'cholesterol', 'epidemiology', 'obesity', 'diseases', 'science', 'funding', 'psychology', 'conferences', 'conventions', 'hypertension', 'adults', 'company', 'kaiser', 'permanente', 'kaiser', 'permanente', 'kaiser', 'permanente', 'national', 'institutes', 'health', 'national', 'institutes', 'health', 'kaiser', 'permanente', 'kaiser', 'permanente', 'national', 'institutes', 'health', 'national', 'institutes', 'health', 'organization', 'national', 'institutes', 'health', 'national', 'institutes', 'health', 'kaiser', 'permanente', 'national', 'institutes', 'health', 'national', 'institutes', 'health', 'kaiser', 'permanente', 'national', 'institutes', 'health', 'national', 'institutes', 'health', 'naics', 'direct', 'health', 'medical', 'insurance', 'carriers', 'sic', 'hospital', 'medical', 'service', 'plans', 'health', 'care', 'hospitalshealth', 'care', 'hospitals', 'health', 'departments', 'television', 'hospitals', 'epidemiology', 'managed', 'care', 'organizations', 'psychology', 'insurance', 'health', 'maintenance', 'organizations', 'tampa', 'fl', 'usa', 'birmingham', 'al', 'usa', 'chicago', 'il', 'usa', 'florida', 'usa', 'alabama', 'usa', 'united', 'states', 'kaiser', 'permanente', 'study', 'obese', 'hostile', 'depressed', 'try', 'watching', 'tv']\n"
     ]
    }
   ],
   "source": [
    "## Remove Stopwords using a custom stopword list\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \n",
    "    ## Open stop words text file and save to stop_set variable\n",
    "    with open(\"stop_words.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        f.close()\n",
    "\n",
    "    ## The stopword list comes from the Terrier pacakge with 733 words and another 86 custom terms: \n",
    "    ## https://github.com/kavgan/stop-words/blob/master/terrier-stop.txt\n",
    "    ## https://github.com/kavgan/stop-words/blob/master/minimal-stop.txt\n",
    "    \n",
    "    ## Other stopword list options can be reviewed here:\n",
    "    ## https://medium.com/towards-artificial-intelligence/stop-the-stopwords-using-different-python-libraries-ffa6df941653\n",
    "\n",
    "    ## Remove stop words from token_list\n",
    "    ## https://stackoverflow.com/questions/29771168/how-to-remove-words-from-a-list-in-python\n",
    "    token_nostop_list = [word for word in text if word not in stop_set]\n",
    "        \n",
    "\n",
    "    return token_nostop_list\n",
    "\n",
    "## New column \"no_stop\" is formed from applying pre_process function to each item in the \"content\" column in dataframe\n",
    "final_df['no_stop'] = final_df['tokens'].apply(lambda x:remove_stopwords(x))\n",
    "\n",
    "\n",
    "print(final_df['no_stop'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bigram and Trigram Tokens from non-stop word data\n",
    "\n",
    "\"Phrase modeling is another approach to learning combinations of tokens that together represent meaningful multi-word concepts. We can develop phrase models by looping over the the words in our corpus and looking for words that co-occur (i.e., appear one after another) together much more frequently than you would expect them to by random chance. The formula our phrase models will use to determine whether two tokens $A$ and $B$ constitute a phrase is:\n",
    "\n",
    "\n",
    "                                                count(AB) - count_{min}\n",
    "                                                - - - - - - - - - - - -   * N > threshold\n",
    "                                                count(A) * count(B)\n",
    "\n",
    "* count(A) is the number of times token $A$ appears in the corpus\n",
    "* count(B) is the number of times token $B$ appears in the corpus\n",
    "* count(AB) is the number of times the tokens $A\\ B$ appear in the corpus in order\n",
    "* N is the total size of the corpus vocabulary\n",
    "* count_{min} is a user-defined parameter to ensure that accepted phrases occur a minimum number of times\n",
    "* threshold is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase\n",
    "\n",
    "Once our phrase model has been trained on our corpus, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n",
    "\n",
    "Phrase modeling is superficially similar to named entity detection in that you would expect named entities to become phrases in the model (so new york would become new_york). But you would also expect multi-word expressions that represent common concepts, but aren't specifically named entities (such as happy hour) to also become phrases in the model.\n",
    "\n",
    "We turn to the indispensible gensim library to help us with phrase modeling — the Phrases class in particular.\"\n",
    "\n",
    "Text from: <https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the \"no_stop\" column in dataframe to a list to use in the build_bigrams_trigrams() function\n",
    "\n",
    "def convert_nostop_to_list(final_df):\n",
    "    \n",
    "    nostop_list = []\n",
    "    \n",
    "    nostop_list = final_df['no_stop'].tolist()\n",
    "\n",
    "    return nostop_list\n",
    "\n",
    "nostop_list = convert_nostop_to_list(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Bigram and Trigram Tokens from non-stop word data, and then compare to stopword\n",
    "\n",
    "def build_bigrams_trigrams(text):\n",
    "#     print(\"This is the text:\", text)\n",
    "#     print(\"---------------------------------------\")\n",
    "\n",
    "    \n",
    "    ##Building Bigram & Trigram Models\n",
    "    ##higher threshold fewer phrases.\n",
    "    bigram = gensim.models.Phrases(text, min_count=5, threshold=100) \n",
    "    ## min_count: Ignore all words and bigrams with total collected count lower than this value.\n",
    "    ## threshold: Represent a score threshold for forming the phrases (higher means fewer phrases).\n",
    "    trigram = gensim.models.Phrases(bigram[text], threshold=100)\n",
    "#     print(bigram)\n",
    "#     print(trigram)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "#     print(bigram_mod)\n",
    "#     print(trigram_mod)\n",
    "\n",
    "    return bigram_mod, trigram_mod\n",
    "\n",
    "bigram_mod, trigram_mod = build_bigrams_trigrams(nostop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['kaiser_permanente', 'study', 'obese', 'hostile', 'depressed', 'try', 'watching_tv', 'kaiser_permanente', 'study', 'obese', 'hostile', 'depressed', 'try', 'watching_tv', 'pr_newswire', 'march', 'friday', 'eastern_time', 'pr_newswire', 'association', 'section', 'lifestyle', 'length_words', 'federally', 'funded', 'kaiser_permanente', 'study', 'found', 'link', 'watching_television', 'obesity', 'hostility', 'depression', 'heavy', 'tv', 'watchers', 'found', 'practice', 'poor', 'health', 'habits', 'associated', 'heart', 'disease', 'smoking', 'study', 'determine', 'tv_viewing', 'actually', 'causes', 'heart', 'attacks', 'heart', 'disease', 'results', 'study', 'presented', 'today', 'kaiser_permanente', 'physician', 'lead_investigator', 'stephen_sidney', 'th_annual', 'conference', 'cardiovascular', 'disease', 'epidemiology', 'prevention', 'tampa', 'fla', 'funded', 'national', 'institutes', 'health', 'cardia', 'study', 'examined', 'television_viewing', 'cardiovascular', 'risk', 'factors', 'young', 'adults', 'cities', 'bi', 'racial', 'population', 'young', 'men', 'women', 'ages', 'years', 'old', 'birmingham_ala', 'chicago', 'minneapolis', 'oakland', 'examined', 'wide', 'variety', 'potential', 'cardiovascular', 'risk', 'factors', 'study', 'group', 'included', 'black', 'men', 'white', 'men', 'black', 'women', 'white', 'women', 'study', 'subjects', 'questioned', 'number', 'hours', 'tv', 'watched', 'day', 'according', 'sidney', 'kaiser_permanente', 'division', 'research', 'oakland', 'who', 'reported', 'watching_tv', 'hours', 'day', 'twice', 'likely', 'cigarette', 'smokers', 'physically', 'inactive', 'who', 'reported', 'watching_tv', 'hour', 'day', 'heavy', 'tv', 'viewers', 'nearly', 'twice', 'likely', 'lighter', 'viewers', 'high', 'score', 'questionnaire', 'measuring', 'hostility', 'likely', 'obese', 'likely', 'high', 'score', 'questionnaire', 'measuring', 'depression', 'sidney', 'reported', 'twenty', 'percent', 'study', 'group', 'reported', 'watching', 'hours', 'television', 'day', 'tv_viewing', 'inconsistently', 'high', 'blood', 'pressure', 'abnormal', 'blood', 'lipids', 'measurements', 'sidney', 'tv_viewing', 'potentially', 'modifiable', 'behavior', 'significantly', 'associated', 'obesity', 'adverse', 'habits', 'adverse', 'psychological', 'measures', 'sidney', 'study', 'determine', 'tv_viewing', 'causally', 'outcomes', 'kaiser_permanente', 'nation', 'largest', 'hmo', 'care', 'million', 'americans', 'kaiser_permanente', 'study', 'obese', 'hostile', 'depressed', 'try', 'watching_tv', 'contact', 'beverly', 'hayon', 'kaiser_permanente', 'subject', 'obesity', 'cardiovascular', 'disease', 'depression', 'diseases_disorders', 'health', 'departments', 'heart', 'disease', 'investigations', 'medicine', 'health', 'press_releases', 'research', 'institutes', 'research', 'reports', 'smoking', 'women', 'cholesterol', 'epidemiology', 'obesity', 'diseases', 'science', 'funding', 'psychology', 'conferences_conventions', 'hypertension', 'adults', 'company', 'kaiser_permanente', 'kaiser_permanente', 'kaiser_permanente', 'national', 'institutes', 'health', 'national', 'institutes', 'health', 'kaiser_permanente', 'kaiser_permanente', 'national', 'institutes', 'health', 'national', 'institutes', 'health', 'organization', 'national', 'institutes', 'health', 'national', 'institutes', 'health', 'kaiser_permanente', 'national', 'institutes', 'health', 'national', 'institutes', 'health', 'kaiser_permanente', 'national', 'institutes', 'health', 'national', 'institutes', 'health', 'naics_direct', 'health', 'medical', 'insurance_carriers', 'sic_hospital', 'medical', 'service', 'plans', 'health', 'care', 'hospitalshealth', 'care', 'hospitals', 'health', 'departments', 'television', 'hospitals', 'epidemiology', 'managed', 'care', 'organizations', 'psychology', 'insurance', 'health', 'maintenance', 'organizations', 'tampa', 'fl', 'usa', 'birmingham_al', 'usa', 'chicago_il', 'usa', 'florida', 'usa', 'alabama', 'usa', 'united_states', 'kaiser_permanente', 'study', 'obese', 'hostile', 'depressed', 'try', 'watching_tv']]\n"
     ]
    }
   ],
   "source": [
    "def make_bigrams(text, bigram_mod):\n",
    "    bigram_token = []\n",
    "    bigram_token.append(bigram_mod[text])\n",
    "    \n",
    "    return bigram_token\n",
    "\n",
    "final_df['bigrams']  = final_df['no_stop'].apply(lambda x:make_bigrams(x, bigram_mod))\n",
    "\n",
    "print(final_df['bigrams'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['kaiser_permanente', 'study', 'obese', 'hostile', 'depressed', 'try', 'watching_tv', 'kaiser_permanente', 'study', 'obese', 'hostile', 'depressed', 'try', 'watching_tv', 'pr_newswire', 'march', 'friday', 'eastern_time', 'pr_newswire', 'association', 'section', 'lifestyle', 'length_words', 'federally', 'funded', 'kaiser_permanente', 'study', 'found', 'link', 'watching_television', 'obesity', 'hostility', 'depression', 'heavy', 'tv', 'watchers', 'found', 'practice', 'poor', 'health', 'habits', 'associated', 'heart', 'disease', 'smoking', 'study', 'determine', 'tv_viewing', 'actually', 'causes', 'heart', 'attacks', 'heart', 'disease', 'results', 'study', 'presented', 'today', 'kaiser_permanente', 'physician', 'lead_investigator', 'stephen_sidney', 'th_annual', 'conference', 'cardiovascular', 'disease', 'epidemiology', 'prevention', 'tampa', 'fla', 'funded', 'national', 'institutes', 'health', 'cardia', 'study', 'examined', 'television_viewing', 'cardiovascular', 'risk', 'factors', 'young', 'adults', 'cities', 'bi', 'racial', 'population', 'young', 'men', 'women', 'ages', 'years', 'old', 'birmingham_ala_chicago', 'minneapolis_oakland', 'examined', 'wide', 'variety', 'potential', 'cardiovascular', 'risk', 'factors', 'study', 'group', 'included', 'black', 'men', 'white', 'men', 'black', 'women', 'white', 'women', 'study', 'subjects', 'questioned', 'number', 'hours', 'tv', 'watched', 'day', 'according', 'sidney', 'kaiser_permanente_division', 'research', 'oakland', 'who', 'reported', 'watching_tv', 'hours', 'day', 'twice_likely', 'cigarette', 'smokers', 'physically', 'inactive', 'who', 'reported', 'watching_tv', 'hour', 'day', 'heavy', 'tv', 'viewers', 'nearly', 'twice_likely', 'lighter', 'viewers', 'high', 'score', 'questionnaire', 'measuring', 'hostility', 'likely', 'obese', 'likely', 'high', 'score', 'questionnaire', 'measuring', 'depression', 'sidney', 'reported', 'twenty', 'percent', 'study', 'group', 'reported', 'watching', 'hours', 'television', 'day', 'tv_viewing', 'inconsistently', 'high', 'blood', 'pressure', 'abnormal', 'blood', 'lipids', 'measurements', 'sidney', 'tv_viewing', 'potentially', 'modifiable', 'behavior', 'significantly', 'associated', 'obesity', 'adverse', 'habits', 'adverse', 'psychological', 'measures', 'sidney', 'study', 'determine', 'tv_viewing', 'causally', 'outcomes', 'kaiser_permanente', 'nation', 'largest', 'hmo', 'care', 'million', 'americans', 'kaiser_permanente', 'study', 'obese', 'hostile', 'depressed', 'try', 'watching_tv', 'contact', 'beverly', 'hayon', 'kaiser_permanente', 'subject', 'obesity', 'cardiovascular', 'disease', 'depression', 'diseases_disorders', 'health', 'departments', 'heart', 'disease', 'investigations', 'medicine', 'health', 'press_releases', 'research', 'institutes', 'research', 'reports', 'smoking', 'women', 'cholesterol', 'epidemiology', 'obesity', 'diseases', 'science', 'funding', 'psychology', 'conferences_conventions', 'hypertension', 'adults', 'company', 'kaiser_permanente', 'kaiser_permanente', 'kaiser_permanente', 'national', 'institutes', 'health', 'national', 'institutes', 'health', 'kaiser_permanente', 'kaiser_permanente', 'national', 'institutes', 'health', 'national', 'institutes', 'health', 'organization', 'national', 'institutes', 'health', 'national', 'institutes', 'health', 'kaiser_permanente', 'national', 'institutes', 'health', 'national', 'institutes', 'health', 'kaiser_permanente', 'national', 'institutes', 'health', 'national', 'institutes', 'health', 'naics_direct', 'health', 'medical', 'insurance_carriers_sic_hospital', 'medical', 'service', 'plans', 'health', 'care', 'hospitalshealth', 'care', 'hospitals', 'health', 'departments', 'television', 'hospitals', 'epidemiology', 'managed', 'care', 'organizations', 'psychology', 'insurance', 'health', 'maintenance', 'organizations', 'tampa', 'fl', 'usa', 'birmingham_al', 'usa', 'chicago_il', 'usa', 'florida', 'usa', 'alabama', 'usa', 'united_states', 'kaiser_permanente', 'study', 'obese', 'hostile', 'depressed', 'try', 'watching_tv']]\n"
     ]
    }
   ],
   "source": [
    "def make_trigrams(text, trigram_mod, bigram_mod ):\n",
    "    trigram_token = []\n",
    "    trigram_token.append(trigram_mod[bigram_mod[text]])\n",
    "    return trigram_token\n",
    "\n",
    "final_df['trigrams']  = final_df['no_stop'].apply(lambda x:make_trigrams(x, trigram_mod, bigram_mod))\n",
    "print(final_df['trigrams'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataframe with specific columns and send to csv for review\n",
    "test_df = pd.DataFrame(final_df, columns = ['tokens','no_stop','bigrams','trigrams'])\n",
    "test_df.to_csv('output/processing/nostop_ngrams.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Spacy for Parts of Speech Tagging \n",
    "\n",
    "Resource(s): \n",
    "<https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parts_of_speech(texts):\n",
    "    \n",
    "    parts_dict = {}\n",
    "    nlp = spacy.load(r'C:\\Users\\keg827\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.3.1')\n",
    "    #nlp = spacy.load('C:\\Users\\keg827\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.3.1', disable=['parser', 'ner'])\n",
    "    nlp.max_length = 2700000 ## or any large value, as long as you don't run out of RAM\n",
    "                    \n",
    "    for doc in texts:\n",
    "        ## Remove the commas between tokens, and apply spacy\n",
    "        new_doc = nlp(\" \".join(doc))\n",
    "        ## Create a list for token and the token's part of speech \n",
    "        token_text = [token.orth_ for token in new_doc]\n",
    "        token_pos = [token.pos_ for token in new_doc]\n",
    "        ## Zip the two lists into a dictionary\n",
    "        parts_dict= dict(zip(token_text, token_pos))\n",
    "  \n",
    "    return parts_dict\n",
    "\n",
    "final_df['parts_of_speech']  = final_df['trigrams'].apply(lambda x:parts_of_speech(x))\n",
    "\n",
    "## Review Spacy's parts of speech here: https://spacy.io/api/annotation\n",
    "## E008 Text Length Exceeds Maximum Error: https://datascience.stackexchange.com/questions/38745/increasing-spacy-max-nlp-limit/55725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataframe with specific columns and send to csv for review\n",
    "test_df = pd.DataFrame(final_df, columns = ['tokens','no_stop','bigrams','trigrams', 'parts_of_speech'])\n",
    "test_df.to_csv('output/processing/nostop_ngrams_partsofspeech.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize the Data\n",
    "\n",
    "\"...lemmatization is a lot more powerful. It looks beyond word reduction and considers a language’s full vocabulary to apply a morphological analysis to words, aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.\"\n",
    "\n",
    "Resource(s):\n",
    "* <https://github.com/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb>\n",
    "* <https://www.geeksforgeeks.org/python-lemmatization-approaches-with-examples/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This step can take some time to complete\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']):\n",
    "    lemmatize = []\n",
    "    nlp = spacy.load(r'C:\\Users\\keg827\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.3.1')\n",
    "    #nlp = spacy.load('C:\\Users\\keg827\\AppData\\Local\\Continuum\\anaconda3\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.3.1', disable=['parser', 'ner'])\n",
    "    nlp.max_length = 2700000 #or any large value, as long as you don't run out of RAM\n",
    "    \n",
    "    for doc in texts:\n",
    "        ## Remove the commas between tokens, and apply spacy\n",
    "        new_doc = nlp(\" \".join(doc))\n",
    "        ## Add lemmatized tokens to the list if token's part of speech is in our allowed list \n",
    "        lemmatize.append([token.lemma_ for token in new_doc if token.pos_ in allowed_postags])\n",
    "       \n",
    "    return lemmatize\n",
    "\n",
    "final_df['lemmatize']  = final_df['trigrams'].apply(lambda x:lemmatization(x, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV', 'PROPN']))\n",
    "\n",
    "## Resources\n",
    "## pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.0.tar.gz\n",
    "## https://stackoverflow.com/questions/54334304/spacy-cant-find-model-en-core-web-sm-on-windows-10-and-python-3-5-3-anacon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save work to CSV and inspect as needed\n",
    "with open(r\"output/processing/nostop_ngrams_partsofspeech_lemmatize.csv\", 'w', encoding='utf-8') as file:\n",
    "    test_df = pd.DataFrame(final_df, columns = ['tokens','no_stop','bigrams','trigrams', 'parts_of_speech', 'lemmatize'])\n",
    "    test_df.to_csv(file, index=False, line_terminator='\\n')\n",
    "    file.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the \"lemmatize\" column in dataframe to a list to provide to the get_gensim_corpus_dictionary() function\n",
    "\n",
    "def convert_lemmatize_to_list(final_df):\n",
    "     \n",
    "    texts_out = final_df['lemmatize'].tolist()\n",
    "\n",
    "    return texts_out\n",
    "\n",
    "texts_out = convert_lemmatize_to_list(final_df)\n",
    "\n",
    "## Inspect output as needed\n",
    "# print(type(texts_out))\n",
    "# print(texts_out)\n",
    "# print(texts_out[1])\n",
    "# print(type(texts_out[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "## Convert the output to a list of lists, not a LIST of lists of lists...which is what it was...\n",
    "## This will be your final text data which will be used in the gensim topic modeling! \n",
    "\n",
    "texts_out_2 = [item for sublist in texts_out for item in sublist]\n",
    "\n",
    "## Inspect the output as needed\n",
    "print(type(texts_out_2))\n",
    "# print(texts_out_2[1])\n",
    "# print(\"----------------------------------------\")\n",
    "# print(texts_out_2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the list as a .pkl file\n",
    "\n",
    "file_name = \"output/processing/texts_out_2.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(texts_out_2, open_file, protocol=4)\n",
    "open_file.close()\n",
    "\n",
    "## Resources\n",
    "## https://www.kite.com/python/answers/how-to-save-and-read-a-list-in-python\n",
    "## https://stackoverflow.com/questions/25843698/valueerror-unsupported-pickle-protocol-3-python2-pickle-can-not-load-the-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
